{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-10dc9a40815c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-10dc9a40815c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    -m scripts.train --recipe-path recipes/mlp_synthetic-Shay.yml \\\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "-m scripts.train --recipe-path recipes/mlp_synthetic-Shay.yml \\\n",
    "  --p-z nagano <Test>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4b552f5614c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import chainer\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from chainer.training import triggers\n",
    "\n",
    "from lib import models, dataset\n",
    "\n",
    "from lib.miscs.hyperparams import Hyperparams\n",
    "from lib.miscs import pathorganizer\n",
    "from lib.miscs.utils import assetsdir, datadir\n",
    "\n",
    "from lib.miscs.logutils import logger_manager\n",
    "logger = logger_manager.register(__name__)\n",
    "\n",
    "TEST_BATCH_SIZE = 50\n",
    "\n",
    "\n",
    "def get_model(hpt):\n",
    "    data_shape = dataset.get_data_shape(hpt.dataset.type, **hpt.dataset)\n",
    "    encoder = models.make_encoder(\n",
    "        hpt.model.type, hpt.model.p_z, data_shape=data_shape, **hpt.model)\n",
    "    decoder = models.make_decoder(\n",
    "        hpt.model.type, hpt.model.p_x, data_shape=data_shape, **hpt.model)\n",
    "    prior = models.make_prior(hpt.model.type, hpt.model.p_z, **hpt.model)\n",
    "\n",
    "    avg_elbo_loss = models.AvgELBOLoss(\n",
    "        encoder, decoder, prior, beta=hpt.loss.beta, k=hpt.loss.k)\n",
    "    return avg_elbo_loss\n",
    "\n",
    "\n",
    "def evaluate(hpt, train, test, avg_elbo_loss):\n",
    "    if hpt.dataset.type in ('mnist', 'breakout'):\n",
    "        with chainer.using_config('train', False), \\\n",
    "                chainer.using_config('enable_backprop', False):\n",
    "\n",
    "            test_iter = chainer.iterators.SerialIterator(\n",
    "                test, TEST_BATCH_SIZE, repeat=False, shuffle=False)\n",
    "\n",
    "            logger.info('calculate test ELBO/LL')\n",
    "            test_k = 500\n",
    "            test_elbo = 0\n",
    "            test_ll = 0\n",
    "            for batch in tqdm(test_iter,\n",
    "                              total=int(np.ceil(len(test) / TEST_BATCH_SIZE))):\n",
    "                test_elbo_, test_ll_ = avg_elbo_loss.get_elbo(\n",
    "                    avg_elbo_loss.xp.asarray(batch), k=test_k, with_ll=True)\n",
    "                test_elbo += test_elbo_.array\n",
    "                test_ll += test_ll_.array\n",
    "            test_elbo /= np.ceil(len(test) / TEST_BATCH_SIZE)\n",
    "            test_ll /= np.ceil(len(test) / TEST_BATCH_SIZE)\n",
    "        return {\n",
    "            'Test AvgELBO': float(test_elbo),\n",
    "            'Test LL': float(test_ll),\n",
    "        }\n",
    "\n",
    "    elif hpt.dataset.type == 'synthetic':\n",
    "        from lib.dataset import binary_tree\n",
    "        from lib import functions\n",
    "\n",
    "        with chainer.using_config('train', False), \\\n",
    "                chainer.using_config('enable_backprop', False):\n",
    "            z = avg_elbo_loss.encoder(train.data).mean\n",
    "            if hpt.dataset.dataset_randomness != -1:\n",
    "                z_prob = avg_elbo_loss.encoder(np.array(train[:])).mean\n",
    "\n",
    "        N = len(train.data)\n",
    "        hamming_dists = np.zeros((N, N))\n",
    "        euclid_dists = np.zeros((N, N))\n",
    "        z_dists = np.zeros((N, N))\n",
    "        z_dists_prob = np.zeros((N, N))\n",
    "        if hpt.model.p_z == 'nagano':\n",
    "            comparator = lambda x, y: functions.lorentz_distance(\n",
    "                x[None, :], y[None, :]).array[0]\n",
    "        else:\n",
    "            comparator = lambda x, y: np.sqrt(((x.array - y.array) ** 2).sum())\n",
    "\n",
    "        for i in range(len(train)):\n",
    "            for j in range(i):\n",
    "                hamming_dists[i, j] = binary_tree.hamming_distance(\n",
    "                    train.data[i], train.data[j])\n",
    "                euclid_dists[i, j] = binary_tree.euclid_distance(\n",
    "                    train.data[i], train.data[j])\n",
    "                z_dists[i, j] = comparator(z[i], z[j])\n",
    "                z_dists_prob[i, j] = comparator(z_prob[i], z_prob[j])\n",
    "\n",
    "        filt_ = np.fromfunction(lambda i, j: i > j, shape=hamming_dists.shape)\n",
    "        return {\n",
    "            'corr-with-hamming': np.corrcoef(\n",
    "                hamming_dists[filt_], z_dists[filt_])[0, 1],\n",
    "            'corr-with-euclid': np.corrcoef(\n",
    "                euclid_dists[filt_], z_dists[filt_])[0, 1],\n",
    "            'corr-with-hamming-noise': np.corrcoef(\n",
    "                hamming_dists[filt_], z_dists_prob[filt_])[0, 1],\n",
    "            'corr-with-euclid-noise': np.corrcoef(\n",
    "                euclid_dists[filt_], z_dists_prob[filt_])[0, 1]\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        raise AttributeError\n",
    "\n",
    "\n",
    "def visualize(hpt, train, test, avg_elbo_loss):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from lib.miscs import plot_utils\n",
    "    from lib import functions\n",
    "\n",
    "    if hpt.dataset.type in ('mnist', 'breakout'):\n",
    "        def save_images(x, filename):\n",
    "            fig, ax = plt.subplots(3, 3, figsize=(9, 9), dpi=100)\n",
    "            for ai, xi in zip(ax.flatten(), x):\n",
    "                if hpt.dataset.type == 'mnist':\n",
    "                    ai.imshow(xi.reshape(28, 28))\n",
    "                elif hpt.dataset.type == 'breakout':\n",
    "                    ai.imshow(xi.reshape(80, 80) / 2 + 0.5)\n",
    "            fig.savefig(filename)\n",
    "\n",
    "        avg_elbo_loss.to_cpu()\n",
    "        train_ind = [1, 3, 5, 10, 2, 0, 13, 15, 17]\n",
    "        x = chainer.Variable(np.asarray(train[train_ind]))\n",
    "        with chainer.using_config('train', False), chainer.no_backprop_mode():\n",
    "            x1 = avg_elbo_loss.decoder(\n",
    "                avg_elbo_loss.encoder(x).mean, n_batch_axes=1).mean\n",
    "        save_images(x.array, (po.imagesdir() / 'train').as_posix())\n",
    "        save_images(\n",
    "            x1.array, (po.imagesdir() / 'train_reconstructed').as_posix())\n",
    "\n",
    "        test_ind = [3, 2, 1, 18, 4, 8, 11, 17, 61]\n",
    "        x = chainer.Variable(np.asarray(test[test_ind]))\n",
    "        with chainer.using_config('train', False), chainer.no_backprop_mode():\n",
    "            x1 = avg_elbo_loss.decoder(\n",
    "                avg_elbo_loss.encoder(x).mean, n_batch_axes=1).mean\n",
    "        save_images(x.array, (po.imagesdir() / 'test').as_posix())\n",
    "        save_images(\n",
    "            x1.array, (po.imagesdir() / 'test_reconstructed').as_posix())\n",
    "\n",
    "        # draw images from randomly sampled z\n",
    "        z = avg_elbo_loss.prior().sample(9)\n",
    "        x = avg_elbo_loss.decoder(z, n_batch_axes=1).mean\n",
    "        save_images(x.array, (po.imagesdir() / 'sampled').as_posix())\n",
    "\n",
    "    elif hpt.dataset.type == 'synthetic':\n",
    "        from lib.dataset import binary_tree\n",
    "\n",
    "        with chainer.using_config('train', False), \\\n",
    "                chainer.using_config('enable_backprop', False):\n",
    "            z = avg_elbo_loss.encoder(train.data).mean\n",
    "            if hpt.dataset.dataset_randomness != -1:\n",
    "                z_prob = avg_elbo_loss.encoder(np.array(train[:])).mean\n",
    "\n",
    "        if hpt.model.p_z == 'euclid':\n",
    "            z_vis = z.array\n",
    "            if hpt.dataset.dataset_randomness != -1:\n",
    "                z_prob_vis = z_prob.array\n",
    "        elif hpt.model.p_z == 'nagano':\n",
    "            z_vis = functions.h2p(z).array\n",
    "            if hpt.dataset.dataset_randomness != -1:\n",
    "                z_prob_vis = functions.h2p(z_prob).array\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        plot_utils.getfig((4, 4))\n",
    "        for i in range(len(train.data)):\n",
    "            for j in range(i):\n",
    "                if binary_tree.hamming_distance(\n",
    "                        train.data[i], train.data[j]) == 1:\n",
    "                    plt.plot(\n",
    "                        [z_vis[i, 0], z_vis[j, 0]], [z_vis[i, 1], z_vis[j, 1]],\n",
    "                        lw=1, color='gray', zorder=1)\n",
    "        plt.scatter(\n",
    "            z_vis[:, 0], z_vis[:, 1], s=(300 / train.data.sum(axis=-1) ** 2),\n",
    "            c='#F15A29', zorder=10)\n",
    "        plt.scatter(0, 0, s=100, c='magenta', zorder=20, marker='x')\n",
    "\n",
    "        if hpt.dataset.dataset_randomness != -1:\n",
    "            plt.scatter(\n",
    "                z_prob_vis[:, 0], z_prob_vis[:, 1],\n",
    "                s=10, c='#4B489E', zorder=5)\n",
    "\n",
    "        plt.box('off')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        plt.savefig(\n",
    "            (po.imagesdir() / 'embedding.pdf').as_posix(),\n",
    "            bbox_inches='tight')\n",
    "        plt.savefig(\n",
    "            (po.imagesdir() / 'embedding.png').as_posix(),\n",
    "            bbox_inches='tight', dpi=400)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def main(hpt):\n",
    "\n",
    "    logger.info('build model')\n",
    "    avg_elbo_loss = get_model(hpt)\n",
    "    if hpt.general.gpu >= 0:\n",
    "        avg_elbo_loss.to_gpu(hpt.general.gpu)\n",
    "\n",
    "    logger.info('setup optimizer')\n",
    "    if hpt.optimizer.type == 'adam':\n",
    "        optimizer = chainer.optimizers.Adam(alpha=hpt.optimizer.lr)\n",
    "    optimizer.setup(avg_elbo_loss)\n",
    "\n",
    "    logger.info('load dataset')\n",
    "    train, valid, test = dataset.get_dataset(hpt.dataset.type, **hpt.dataset)\n",
    "\n",
    "    if hpt.general.test:\n",
    "        train, _ = chainer.datasets.split_dataset(train, 100)\n",
    "        valid, _ = chainer.datasets.split_dataset(valid, 100)\n",
    "        test, _ = chainer.datasets.split_dataset(test, 100)\n",
    "\n",
    "    train_iter = chainer.iterators.SerialIterator(\n",
    "        train, hpt.training.batch_size)\n",
    "    valid_iter = chainer.iterators.SerialIterator(\n",
    "        valid, hpt.training.batch_size, repeat=False, shuffle=False)\n",
    "\n",
    "    logger.info('setup updater/trainer')\n",
    "    updater = training.updaters.StandardUpdater(\n",
    "        train_iter, optimizer,\n",
    "        device=hpt.general.gpu, loss_func=avg_elbo_loss)\n",
    "\n",
    "    if not hpt.training.early_stopping:\n",
    "        trainer = training.Trainer(\n",
    "            updater, (hpt.training.iteration, 'iteration'),\n",
    "            out=po.namedir(output='str'))\n",
    "    else:\n",
    "        trainer = training.Trainer(\n",
    "            updater, triggers.EarlyStoppingTrigger(\n",
    "                monitor='validation/main/loss',\n",
    "                patients=5,\n",
    "                max_trigger=(hpt.training.iteration, 'iteration')\n",
    "            ), out=po.namedir(output='str'))\n",
    "\n",
    "    if hpt.training.warm_up != -1:\n",
    "        time_range = (0, hpt.training.warm_up)\n",
    "        trainer.extend(\n",
    "            extensions.LinearShift('beta', value_range=(0.1, hpt.loss.beta),\n",
    "                                   time_range=time_range,\n",
    "                                   optimizer=avg_elbo_loss))\n",
    "\n",
    "    trainer.extend(extensions.Evaluator(\n",
    "        valid_iter, avg_elbo_loss, device=hpt.general.gpu))\n",
    "    # trainer.extend(extensions.DumpGraph('main/loss'))\n",
    "    trainer.extend(\n",
    "        extensions.snapshot_object(\n",
    "            avg_elbo_loss, 'avg_elbo_loss_snapshot_iter_{.updater.iteration}'),\n",
    "        trigger=(int(hpt.training.iteration / 5), 'iteration'))\n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(extensions.observe_lr())\n",
    "    trainer.extend(extensions.PrintReport([\n",
    "        'epoch', 'iteration', 'main/loss', 'validation/main/loss',\n",
    "        'main/reconstr', 'main/kl_penalty', 'main/beta', 'lr', 'elapsed_time'\n",
    "    ]))\n",
    "    trainer.extend(extensions.ProgressBar())\n",
    "\n",
    "    logger.info('run training')\n",
    "    trainer.run()\n",
    "\n",
    "    logger.info('save last model')\n",
    "    extensions.snapshot_object(\n",
    "        avg_elbo_loss, 'avg_elbo_loss_snapshot_iter_{.updater.iteration}'\n",
    "    )(trainer)\n",
    "\n",
    "    logger.info('evaluate')\n",
    "    metrics = evaluate(hpt, train, test, avg_elbo_loss)\n",
    "    for metric_name, metric in metrics.items():\n",
    "        logger.info('{}: {:.4f}'.format(metric_name, metric))\n",
    "\n",
    "    if hpt.general.noplot:\n",
    "        return metrics\n",
    "\n",
    "    logger.info('visualize images')\n",
    "    visualize(hpt, train, test, avg_elbo_loss)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    current_file = pathlib.Path(__file__)\n",
    "    hpt = Hyperparams(current_file.parents[1] / 'recipes/mlp_default.yml')\n",
    "    hpt = hpt.parse_args(program_name=current_file.name)\n",
    "\n",
    "    logger.info(hpt.summary())\n",
    "\n",
    "    po = pathorganizer.PathOrganizer(\n",
    "        root=assetsdir, name=hpt.general.name, datadir=datadir)\n",
    "\n",
    "    with (po.namedir() / 'option.yml').open('w') as f:\n",
    "        hpt.dump(f)\n",
    "\n",
    "    if hpt.general.num_experiments == 1:\n",
    "        main(hpt)\n",
    "    else:\n",
    "        metrics = []\n",
    "        for experiment_idx in range(hpt.general.num_experiments):\n",
    "            logger.info('Trial [{}/{}] start'.format(\n",
    "                experiment_idx + 1, hpt.general.num_experiments))\n",
    "            if experiment_idx > 0:\n",
    "                hpt['general']['noplot'] = True\n",
    "            metrics.append(main(hpt))\n",
    "        metrics = pd.DataFrame(metrics)\n",
    "        logger.info('Total result:')\n",
    "        for name in metrics.columns:\n",
    "            logger.info('{}: {:.4f} \\\\pm {:.4f}'.format(\n",
    "                name, metrics[name].mean(), metrics[name].std()))\n",
    "        metrics.to_csv((po.logsdir() / 'metrics.tsv').as_posix(), sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.48.2-py2.py3-none-any.whl (68 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.48.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaya\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.1.1-cp37-cp37m-win_amd64.whl (9.4 MB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\shaya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (1.19.1)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\shaya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shaya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pytz, pandasNote: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed pandas-1.1.1 pytz-2020.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaya\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
